services:
  pyspark:
    build: .
    container_name: pyspark-container
    ports:
      - "8889:8888"
    networks:
      - spark-network
    volumes:
      - ./notebooks:/opt/notebooks

  spark-ui:
    image: apache/spark:latest
    container_name: spark-ui-container
    ports:
        - "4040:4040"
    networks:
        - spark-network
    depends_on:
        - pyspark

  airflow-webserver:
    image: apache/airflow:2.10.4
    container_name: airflow-webserver-container
    environment:
        - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
        - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
        - AIRFLOW__CELERY__BROKER_URL=redis://:@redis:6379/0
        - AIRFLOW__CORE__FERNET_KEY=  # Add a generated Fernet key here
    ports:
        - "8080:8080"
    networks:
        - spark-network
    depends_on:
        - redis
        - postgres
    command: airflow webserver  # Explicitly start the webserver
    healthcheck:
        test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
        interval: 30s
        timeout: 10s
        retries: 5
        start_period: 30s
    restart: always

  airflow-scheduler:
    image: apache/airflow:2.10.4
    container_name: airflow-scheduler-container
    environment:
        - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
        - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
        - AIRFLOW__CELERY__BROKER_URL=redis://:@redis:6379/0
    networks:
        - spark-network
    depends_on:
        - airflow-webserver
        - redis
        - postgres
    healthcheck:
        test: ["CMD", "curl", "--fail", "http://localhost:8793/health"]
        interval: 30s
        timeout: 10s
        retries: 5
        start_period: 60s
    restart: always

  airflow-worker:
    image: apache/airflow:2.10.4
    container_name: airflow-worker-container
    environment:
        - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
        - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
        - AIRFLOW__CELERY__BROKER_URL=redis://:@redis:6379/0
    networks:
        - spark-network
    depends_on:
        - airflow-scheduler
    healthcheck:
        test: ["CMD", "curl", "--fail", "http://localhost:8793/health"]
        interval: 30s
        timeout: 10s
        retries: 5
        start_period: 60s
    restart: always

  flower:
    image: apache/airflow:2.10.4
    container_name: airflow-flower-container
    command: celery flower
    environment:
        - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
        - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
        - AIRFLOW__CELERY__BROKER_URL=redis://:@redis:6379/0
    ports:
        - "5555:5555"
    networks:
        - spark-network
    depends_on:
        - redis
    healthcheck:
        test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
        interval: 30s
        timeout: 10s
        retries: 5
        start_period: 60s
    restart: always

  postgres:
    image: postgres:latest
    container_name: postgres-container
    environment:
        POSTGRES_USER: airflow
        POSTGRES_PASSWORD: airflow
        POSTGRES_DB: airflow
    ports:
        - "5432:5432"
    networks:
        - spark-network
    healthcheck:
        test: ["CMD", "pg_isready", "-U", "airflow"]
        interval: 10s
        retries: 5
        start_period: 30s
    restart: always

  redis:
    image: redis:latest
    container_name: redis-container
    networks:
        - spark-network
    healthcheck:
        test: ["CMD", "redis-cli", "ping"]
        interval: 10s
        timeout: 30s
        retries: 50
        start_period: 30s
    restart: always

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin-container
    environment:
        PGADMIN_DEFAULT_EMAIL: admin@example.com
        PGADMIN_DEFAULT_PASSWORD: admin
    ports:
        - "5050:80"
    networks:
        - spark-network
    depends_on:
        - postgres
    healthcheck:
        test: ["CMD", "pg_isready", "-U", "airflow"]
        interval: 30s
        timeout: 10s
        retries: 5
        start_period: 60s
    restart: always

networks:
  spark-network:
    driver: bridge